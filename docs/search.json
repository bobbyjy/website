[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Corey Cassell",
    "section": "",
    "text": "A Portland Oregon based engineer and masters in data science candidate with 7+ years of broad-based experience utilizing advanced data and analytical tools, building production grade metrics and visualizations, while integrating industrial engineering principles to drive continuous improvement for domestic and international customers."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Corey Cassell",
    "section": "Education",
    "text": "Education\nWillamette University\nMasters in Data Science (MSDS) Planned Graduation August 2023\nCalifornia Polytechnical State University, San Luis Obispo\nBachelor of Science, Industrial Engineering"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Corey Cassell",
    "section": "Skills",
    "text": "Skills\nSOFTWARE, PLATFORMS, AND PROGRAMMING\n\nSQL – Accessing, cleaning, and joining high volume data in a fast-paced manufacturing environment.\nR – Crafting visualizations through base\nR and ggplot. Writing web scraping scripts. Tuning and implementing machine learning models.\nTableau – Design, build, implementation of production facing metrics and analytical tools for manager and executive level communication. Improving user access and understanding of production performance to enable better allocation of resources.\nAccess/Excel/VBA – creating automated metrics and scrips to answer emergent questions.\nBASH – Basic file navigation and manipulation. Remote server interaction. Docker file building and deployment.\n\nGitHub – Project management and version control.\nPython – Basic level understanding and capability.\n\nDATA SCIENCE\n\nData Visualization – Using R and Tableau to analyze and interpret business anomalies through uncovering hidden trends.\nData Engineering – Employing the ETL process in the setup, design, and management of PostgreSQL server/database relationship. joining of tables from cross platform databases (Teradata/Oracle/Microsoft SQL Server) and transforming information into useable content.\n\nMachine learning – Leveraging linear regression-based models to predict next day manufacturing performance for more efficient current day resource allocation.\nSurvival Analysis and survival curve plotting to predict product life expectancy"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Corey Cassell",
    "section": "Experience",
    "text": "Experience\nINDUSTRIAL ENGINEER III, BOEING October 2016 – Current\nBOEING PORTLAND, OR, USA – SUPPORT FOR ACTUATION SYSTEMS MANUFACTURING TEAM\n\nTeam focal for analytics and metrics development using Tableau, Access, and Excel platforms\nGeneration of Tableau dashboards through the integration of multiplatform SQL query building\nDevelopment of statistical standard processes for production performance visibility including data cleaning, preparation, analysis, and site leadership level communication\nAutomated daily report design, construction, and support\nMachining cell layout design and implementation\nDetailed production schedule recovery planning\nPlant staffing and machining capacity analysis in dynamic demand environment\n\nBOEING SHEFFIELD, UK – 7 MONTHS TEMPORARY ASSIGNMENT\n\nUser story design and development for web metric deployment\nOperation level time study analysis enabling targeted problem solving for cycle time reduction\nTraining and support of Sheffield IE team\nLean 3P (production preparation process) workshop support for transmission housings\nThird party supplier capacity and supply chain buffering analysis\nWork statement and movement scenario analysis providing solutions for over capacity cells\nNew product cross site production planning and integration"
  },
  {
    "objectID": "AnotherNewTab.html",
    "href": "AnotherNewTab.html",
    "title": "This is a tab that I am testing",
    "section": "",
    "text": "About this site: you don’t need to know anyting about htis site\n\n1 + 1\n\n[1] 2\n\n#just testing out some changes!!!\n#more changes\n#another test\n\n::: {layout-ncol=2} This fish is pretty neat \n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nWarning in gg_record(dir = \"tidytuesday-temp\", device = \"png\", width = 8, :\nWriting to a folder that already exists. gg_playback may use more files than\nintended!\n\n\nRows: 18215 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): tag_id, study_name\ndbl  (5): event_id, location_long, location_lat, ground_speed, height_above_...\nlgl  (3): visible, algorithm_marked_outlier, manually_marked_outlier\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 101 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): tag_id, animal_id, animal_taxon, animal_reproductive_condition, an...\ndbl  (4): prey_p_month, hrs_indoors, n_cats, age_years\nlgl  (4): hunt, food_dry, food_wet, food_other\ndttm (2): deploy_on_date, deploy_off_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nJoining, by = \"tag_id\"\n\n\n\na"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corey Cassell",
    "section": "",
    "text": "A Portland Oregon based engineer and masters in data science candidate with 7+ years of broad-based experience utilizing advanced data and analytical tools, building production grade metrics and visualizations, while integrating industrial engineering principles to drive continuous improvement for domestic and international customers."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Corey Cassell",
    "section": "Education",
    "text": "Education\nWillamette University\nMasters in Data Science (MSDS) Planned Graduation August 2023\nCalifornia Polytechnical State University, San Luis Obispo\nBachelor of Science, Industrial Engineering"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Corey Cassell",
    "section": "Skills",
    "text": "Skills\nSOFTWARE, PLATFORMS, AND PROGRAMMING\n\nSQL – Accessing, cleaning, and joining high volume data in a fast-paced manufacturing environment.\nR – Crafting visualizations through base\nR and ggplot. Writing web scraping scripts. Tuning and implementing machine learning models.\nTableau – Design, build, implementation of production facing metrics and analytical tools for manager and executive level communication. Improving user access and understanding of production performance to enable better allocation of resources.\nAccess/Excel/VBA – creating automated metrics and scrips to answer emergent questions.\nBASH – Basic file navigation and manipulation. Remote server interaction. Docker file building and deployment.\n\nGitHub – Project management and version control.\nPython – Basic level understanding and capability.\n\nDATA SCIENCE\n\nData Visualization – Using R and Tableau to analyze and interpret business anomalies through uncovering hidden trends.\nData Engineering – Employing the ETL process in the setup, design, and management of PostgreSQL server/database relationship. joining of tables from cross platform databases (Teradata/Oracle/Microsoft SQL Server) and transforming information into useable content.\n\nMachine learning – Leveraging linear regression-based models to predict next day manufacturing performance for more efficient current day resource allocation.\nSurvival Analysis and survival curve plotting to predict product life expectancy"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Corey Cassell",
    "section": "Experience",
    "text": "Experience\nINDUSTRIAL ENGINEER III, BOEING October 2016 – Current\nBOEING PORTLAND, OR, USA – SUPPORT FOR ACTUATION SYSTEMS MANUFACTURING TEAM\n\nTeam focal for analytics and metrics development using Tableau, Access, and Excel platforms\nGeneration of Tableau dashboards through the integration of multiplatform SQL query building\nDevelopment of statistical standard processes for production performance visibility including data cleaning, preparation, analysis, and site leadership level communication\nAutomated daily report design, construction, and support\nMachining cell layout design and implementation\nDetailed production schedule recovery planning\nPlant staffing and machining capacity analysis in dynamic demand environment\n\nBOEING SHEFFIELD, UK – 7 MONTHS TEMPORARY ASSIGNMENT\n\nUser story design and development for web metric deployment\nOperation level time study analysis enabling targeted problem solving for cycle time reduction\nTraining and support of Sheffield IE team\nLean 3P (production preparation process) workshop support for transmission housings\nThird party supplier capacity and supply chain buffering analysis\nWork statement and movement scenario analysis providing solutions for over capacity cells\nNew product cross site production planning and integration"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "As an introductory project aimed at exploring the functionality of Quarto and Github I have replicated an entry from #tidytuesday For this example I am using code from a submission by @ryanahart with (code: https://github.com/curatedmess/TidyTuesday/blob/main/2023/01312023/cats.R)\n\n\n\n\n\n\n\n\n\n\nSimplified the visual into a bar chart to align the information along a common axis and make each cat easier to compare.\n\n\nShow the code\n#\ndf %>% \n  ggplot(aes(distance, reorder(name,distance), fill = distance))+\n  geom_col()+\n  geom_col(data=(df %>% filter(name==\"Dexter2-Tag\")),aes(distance,name),fill=\"red\")+\n  geom_col(data=(df %>% filter(name==\"Neva-Tag\")),aes(distance,name),fill=\"green\")+\n  theme(axis.text = element_text(size = 3),\n        axis.title = element_text(size=6),\n        plot.title = element_text(size = 8),\n        plot.background = element_rect(fill = \"snow1\"),\n        panel.background = element_rect(fill = \"snow1\"),\n        legend.position = 'none')+\n  labs(title = \"Cat Distances Traveled!\",\n       y = \"Cat Names\", x = \"Distance\")"
  },
  {
    "objectID": "Projects.html#project-2",
    "href": "Projects.html#project-2",
    "title": "Projects",
    "section": "Project 2:",
    "text": "Project 2:\nAn example of a KNN model that we are building and improving in class. The objective is to accurately predict the regional origin of a bottle of wine.\n\nUnderstanding most frequently used regional descriptors…\n\n\nShow the code\n#install.packages(\"tm\")\nlibrary(tm) #for reading in text documents\n#install.packages(\"tidytext\")\nlibrary(tidytext) # for cleaning text and sentiments\n#install.packages(\"topicmodels\")\nlibrary(topicmodels) # for topic analysis\n#install.packages(\"janeaustenr\")\nlibrary(janeaustenr) # for free data\nlibrary(dplyr) # for data manipulation\nlibrary(tidyr)\nlibrary(stringr) # for manipulating string/text data\nlibrary(ggplot2) # for pretty graphs\n#install.packages(\"wordcloud\")\nlibrary(wordcloud) #duh\n#install.packages(\"stopwords\")\nlibrary(stopwords)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(stringr)\nlibrary(formatR)\nlibrary(moderndive)\nlibrary(fastDummies)\nlibrary(gridExtra)\n\nwine <- read_rds(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/pinot.rds\")\n\n\n# wine %>% \n#   select(province) %>% \n#   distinct()\n\n# Oregon                \n# California                \n# Burgundy              \n# Marlborough               \n# Casablanca_Valley             \n# New_York\n\nOrWine <- wine %>% filter(province == \"Oregon\") %>% select(description)\nCaWine <- wine %>% filter(province == \"California\") %>% select(description)\nBurWine <- wine %>% filter(province == \"Burgundy\") %>% select(description) \nMarWine <- wine %>% filter(province == \"Marlborough\") %>% select(description)\nCasWine <- wine %>% filter(province == \"Casablanca_Valley\") %>% select(description)\nNewYWine <- wine %>% filter(province == \"New_York\") %>% select(description)\n\nOR <- tibble(txt=OrWine$description)\nCA <- tibble(txtCA=CaWine$description)\nBU <- tibble(txtBU=BurWine$description)\nMA <- tibble(txtMA=MarWine$description)\nCAS <- tibble(txtCAS=CasWine$description)\nNY <- tibble(txtNY=NewYWine$description)\n\nOR<-OR %>% \n  unnest_tokens(word,txt)\nCA<-CA %>% \n  unnest_tokens(word,txtCA)\nBU<-BU %>% \n  unnest_tokens(word,txtBU)\nMA<-MA %>% \n  unnest_tokens(word,txtMA)\nCAS<-CAS %>% \n  unnest_tokens(word,txtCAS)\nNY<-NY %>% \n  unnest_tokens(word,txtNY)\n\ncleaned_OR <- OR %>% \n  anti_join(get_stopwords())\ncleaned_CA <- CA %>% \n  anti_join(get_stopwords())\ncleaned_BU <- BU %>% \n  anti_join(get_stopwords())\ncleaned_MA <- MA %>% \n  anti_join(get_stopwords())\ncleaned_CAS <- CAS %>% \n  anti_join(get_stopwords())\ncleaned_NY <- NY %>% \n  anti_join(get_stopwords())\n\n\ncleaned_OR <- cleaned_OR %>% \n  group_by(word) %>% \n  summarise(word, n = n()) %>% \n  distinct() %>% \n  filter(n>400) %>% \n  arrange(desc(n))\ncleaned_CA <- cleaned_CA %>% \n  group_by(word) %>% \n  summarise(word, n = n()) %>% \n  distinct() %>% \n  filter(n>400) %>% \n  arrange(desc(n))\ncleaned_BU <- cleaned_BU %>% \n  group_by(word) %>% \n  summarise(word, n = n()) %>% \n  distinct() %>% \n  filter(n>400) %>% \n  arrange(desc(n))\ncleaned_MA <- cleaned_MA %>% \n  group_by(word) %>% \n  summarise(word, n = n()) %>% \n  distinct() %>% \n  filter(n>50) %>% \n  arrange(desc(n))\ncleaned_CAS <- cleaned_CAS %>% \n  group_by(word) %>% \n  summarise(word, n = n()) %>% \n  distinct() %>% \n  filter(n>50) %>% \n  arrange(desc(n))\ncleaned_NY <- cleaned_NY %>% \n  group_by(word) %>% \n  summarise(word, n = n()) %>% \n  distinct() %>% \n  filter(n>50) %>% \n  arrange(desc(n))\n\n\n\na<-cleaned_OR %>% ggplot(aes(reorder(word,n), n, fill = n))+\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Oregon Word Descriptors over 400 Occurences\")+\n  #CoreyPlotTheme+\n  theme_bw()+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 3), plot.title = element_text(size=5))\nb<-cleaned_CA %>% ggplot(aes(reorder(word,n), n, fill = n))+\n  geom_col(show.legend = FALSE) +\n    theme_bw()+\n  labs(title = \"California Word Descriptors over 400 Occurences\")+\n  #CoreyPlotTheme+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 3), plot.title = element_text(size=5))\nc<-cleaned_BU %>% ggplot(aes(reorder(word,n), n, fill = n))+\n  geom_col(show.legend = FALSE) +\n    theme_bw()+\n  labs(title = \"Burgondy? Word Descriptors over 400 Occurences\")+\n  #CoreyPlotTheme+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 3), plot.title = element_text(size=5))\nd<-cleaned_MA %>% ggplot(aes(reorder(word,n), n, fill = n))+\n  geom_col(show.legend = FALSE) +\n    theme_bw()+\n  labs(title = \"Malboro? Word Descriptors over 50 Occurences\")+\n  #CoreyPlotTheme+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 3), plot.title = element_text(size=5))\ne<-cleaned_CAS %>% ggplot(aes(reorder(word,n), n, fill = n))+\n  geom_col(show.legend = FALSE) +\n    theme_bw()+\n  labs(title = \"Casa-whatever Word Descriptors over 50 Occurences\")+\n  #CoreyPlotTheme+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 3), plot.title = element_text(size=5))\nf<-cleaned_NY %>% ggplot(aes(reorder(word,n), n, fill = n))+\n  geom_col(show.legend = FALSE) +\n    theme_bw()+\n  labs(title = \"New York Word Descriptors over 50 Occurences\")+\n  #CoreyPlotTheme+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 3), plot.title = element_text(size=5))\n\ngrid.arrange(arrangeGrob(a,b,c,ncol=3), arrangeGrob(d,e,f,ncol=3), nrow=2)\n\n\n\n\n\nShow the code\nwine$chocolate <- ifelse(str_detect(wine$description, regex('chocolate', ignore_case = T)),1,0)\nwine$black <- ifelse(str_detect(wine$description, regex('black', ignore_case = T)),1,0)\nwine$oak <- ifelse(str_detect(wine$description, regex('oak', ignore_case = T)),1,0)\nwine$tart <- ifelse(str_detect(wine$description, regex('tart', ignore_case = T)),1,0)\nwine$oregon <- ifelse(str_detect(wine$description, regex('oregon', ignore_case = T)),1,0)\nwine$aroma <- ifelse(str_detect(wine$description, regex('aroma', ignore_case = T)),1,0)\nwine$palate <- ifelse(str_detect(wine$description, regex('palate', ignore_case = T)),1,0)\nwine$bottling <- ifelse(str_detect(wine$description, regex('bottling', ignore_case = T)),1,0)\nwine$spice <- ifelse(str_detect(wine$description, regex('spice', ignore_case = T)),1,0)\nwine$california <- ifelse(str_detect(wine$description, regex('california', ignore_case = T)),1,0)\nwine$ripe <- ifelse(str_detect(wine$description, regex('ripe', ignore_case = T)),1,0)\nwine$tannin <- ifelse(str_detect(wine$description, regex('tannin', ignore_case = T)),1,0)\nwine$drink <- ifelse(str_detect(wine$description, regex('drink', ignore_case = T)),1,0)\nwine$burgundy <- ifelse(str_detect(wine$description, regex('burgundy', ignore_case = T)),1,0)\nwine$medium <- ifelse(str_detect(wine$description, regex('medium', ignore_case = T)),1,0)\nwine$noir <- ifelse(str_detect(wine$description, regex('noir', ignore_case = T)),1,0) # marlborlal..ish?\nwine$note <- ifelse(str_detect(wine$description, regex('note', ignore_case = T)),1,0)\nwine$now <- ifelse(str_detect(wine$description, regex('now', ignore_case = T)),1,0)\nwine$marlborough <- ifelse(str_detect(wine$description, regex('marlborough', ignore_case = T)),1,0)\nwine$plum <- ifelse(str_detect(wine$description, regex('plum', ignore_case = T)),1,0)\n\nwine$finish <- ifelse(str_detect(wine$description, regex('finish', ignore_case = T)),1,0)\nwine$flavor <- ifelse(str_detect(wine$description, regex('flavor', ignore_case = T)),1,0)\nwine$casa <- ifelse(str_detect(wine$description, regex('casablanca', ignore_case = T)),1,0)\nwine$newyork <- ifelse(str_detect(wine$description, regex('new york', ignore_case = T)),1,0)\nwine$cola <- ifelse(str_detect(wine$description, regex('cola', ignore_case = T)),1,0)\nwine$forward <- ifelse(str_detect(wine$description, regex('forward', ignore_case = T)),1,0)\nwine$estate <- ifelse(str_detect(wine$description, regex('estate', ignore_case = T)),1,0)\nwine$spicy <- ifelse(str_detect(wine$description, regex('spicy', ignore_case = T)),1,0)\nwine$herb <- ifelse(str_detect(wine$description, regex('herb', ignore_case = T)),1,0)\nwine$french <- ifelse(str_detect(wine$description, regex('french', ignore_case = T)),1,0)\nwine$elegant <- ifelse(str_detect(wine$description, regex('elegant', ignore_case = T)),1,0)\nwine$earth <- ifelse(str_detect(wine$description, regex('earth', ignore_case = T)),1,0)\nwine$acidity <- ifelse(str_detect(wine$description, regex('acidity', ignore_case = T)),1,0)\nwine$raspberry <- ifelse(str_detect(wine$description, regex('raspberry', ignore_case = T)),1,0)\n#^just these factors make a .5!\nwine$structure <- ifelse(str_detect(wine$description, regex('structure', ignore_case = T)),1,0)\nwine$wood <- ifelse(str_detect(wine$description, regex('wood', ignore_case = T)),1,0)\nwine$aging <- ifelse(str_detect(wine$description, regex('aging', ignore_case = T)),1,0)\nwine$soft <- ifelse(str_detect(wine$description, regex('soft', ignore_case = T)),1,0)\nwine$firm <- ifelse(str_detect(wine$description, regex('firm', ignore_case = T)),1,0)\n#^just these factors make a .53\nwine$supple <- ifelse(str_detect(wine$description, regex('supple', ignore_case = T)),1,0)\nwine$long <- ifelse(str_detect(wine$description, regex('long', ignore_case = T)),1,0)\nwine$crisp <- ifelse(str_detect(wine$description, regex('crisp', ignore_case = T)),1,0)\nwine$dusty <- ifelse(str_detect(wine$description, regex('dusty', ignore_case = T)),1,0)\n\nwine$cuv <- ifelse(str_detect(wine$description, regex('cuvée', ignore_case = T)),1,0)\nwine$core <- ifelse(str_detect(wine$description, regex('core', ignore_case = T)),1,0)\nwine$leaf <- ifelse(str_detect(wine$description, regex('leaf', ignore_case = T)),1,0)\nwine$rich <- ifelse(str_detect(wine$description, regex('rich', ignore_case = T)),1,0)\nwine$months <- ifelse(str_detect(wine$description, regex('months', ignore_case = T)),1,0)\n#.547^\n\nwine <- wine %>% \n  mutate(points = scale(points, center = T, scale = T)) %>%\n  mutate(price = scale(log(price), center = T, scale = T)) %>% \n  select(-id,-description)\n\n\n\n\nHow does the model perform across provinces?\nAccuracy is very high for Burgundy, California, and Oregon regions. Smaller regions performed substantially worse, potentially due to significantly lower availability of data.\n\n\nShow the code\n#PROJECT 1 Feature Engineering\nset.seed(504)\nwine_index <- createDataPartition(wine$province, p = 0.8, list = FALSE)\ntrain <- wine[ wine_index, ]\ntest <- wine[-wine_index, ]\n\n#10 fold cross validation\ncontrol <- trainControl(\n                           method = \"repeatedcv\",\n                           number = 3\n                           )\n\nfit <- train(province ~ .,\n             data = train, \n             method = \"knn\",#knn\n             tuneLength = 20, #the 30 different values for K\n             metric = \"Kappa\",\n             trControl = control)\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               213          8                 0           3        2\n  California               5        689                 9          15        8\n  Casablanca_Valley        0          0                 8           0        1\n  Marlborough              0          1                 0          10        0\n  New_York                 0          0                 0           1        4\n  Oregon                  20         93                 9          16       11\n                   Reference\nPrediction          Oregon\n  Burgundy              16\n  California           131\n  Casablanca_Valley      1\n  Marlborough            0\n  New_York               0\n  Oregon               399\n\nOverall Statistics\n                                          \n               Accuracy : 0.7908          \n                 95% CI : (0.7705, 0.8101)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6678          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.8950            0.8710                 0.307692\nSpecificity                   0.9798            0.8095                 0.998786\nPos Pred Value                0.8802            0.8040                 0.800000\nNeg Pred Value                0.9825            0.8750                 0.989176\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1273            0.4118                 0.004782\nDetection Prevalence          0.1447            0.5123                 0.005977\nBalanced Accuracy             0.9374            0.8403                 0.653239\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.222222        0.153846        0.7294\nSpecificity                    0.999386        0.999393        0.8677\nPos Pred Value                 0.909091        0.800000        0.7281\nNeg Pred Value                 0.978941        0.986811        0.8684\nPrevalence                     0.026898        0.015541        0.3270\nDetection Rate                 0.005977        0.002391        0.2385\nDetection Prevalence           0.006575        0.002989        0.3276\nBalanced Accuracy              0.610804        0.576619        0.7986\n\n\n\n\nWhich K value is ideal?\nThis graphs kappa performance of the model over a range of K values. Kappa is a measure of model performance against random chance classification. We want to achieve the highest kappa value possible.\n\n\nShow the code\nggplot(fit, metric=\"Kappa\")"
  },
  {
    "objectID": "Visualization.html",
    "href": "Visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "My portfolio! I’ll be storing examples of my work and skillset in this location.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nggsurvplot(fit=km, data=ds,\n           legend = \"bottom\", \n           legend.title = \"Treatment Group\",\n           legend.labs = c(\"Combination\", \"Patch Only\"),\nrisk.table = F,conf.int=T) +\n    labs(\n        title=\"Survival Curve for Pharmaco Study\",\n        x=\"Time to Relapse)\"\n    )"
  },
  {
    "objectID": "Projects.html#project-3-using-tables",
    "href": "Projects.html#project-3-using-tables",
    "title": "Projects",
    "section": "Project 3: Using Tables",
    "text": "Project 3: Using Tables\n\n\nShow the code\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(survival)# a change\nlibrary(survminer)\nlibrary(reReg)\nlibrary(KMsurv)\nlibrary(Ecdat)\nlibrary(ggplot2)\n\n\nThis is a study of Natural vs Anthropogenic soils for\n\n\nShow the code\n#data repo: https://github.com/bobbyjy/MyData\n\n#load dataset into git repo\nthePath=\"https://raw.githubusercontent.com/bobbyjy/MyData/main/\"\n\n#update with file name for project 1\nAnth=read_csv(paste(thePath,\"Anthropogenic.csv\",sep=\"/\"))\n\n#load dataset into git repo\nthePath=\"https://raw.githubusercontent.com/bobbyjy/MyData/main/\"\n\n#update with file name for project 1\nNat=read_csv(paste(thePath,\"Natural.csv\",sep=\"/\"))\n\nAnth<-Anth %>% mutate(anthro=1)\nNat<-Nat %>% mutate(anthro=0)\ncoraldf <- dplyr::combine(Anth,Nat)\n\n#comparing temperature and survival\nTemp <- coraldf %>% select(Temperature, Time, Status)\nkm.Temp =survfit(Surv(Temp$Time,Temp$Status)~Temp$Temperature)\n\np <- ggsurvplot(fit=km.Temp, data=Temp,\n           legend = \"bottom\", risk.table = F,conf.int=F, legend.title = \"Study Temperatures: \") +\n    labs(\n        title=\"Visualizing the Impact of Global Warming on Coral Populations\",\n        subtitle = \"Observing Coral Survival in 26C and 30C Groups\",\n        x=\"Time Until Coral Death (Weeks)\"\n    ) \nsurvdiff(Surv(Temp$Time,Temp$Status)~Temp$Temperature)\n\n\nCall:\nsurvdiff(formula = Surv(Temp$Time, Temp$Status) ~ Temp$Temperature)\n\n                      N Observed Expected (O-E)^2/E (O-E)^2/V\nTemp$Temperature=26 646      572      751      42.9       167\nTemp$Temperature=30 610      607      428      75.3       167\n\n Chisq= 167  on 1 degrees of freedom, p= <2e-16 \n\n\nShow the code\nsummary(km.Temp)\n\n\nCall: survfit(formula = Surv(Temp$Time, Temp$Status) ~ Temp$Temperature)\n\n                Temp$Temperature=26 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    646     137    0.788  0.0161       0.7570        0.820\n    2    509      63    0.690  0.0182       0.6557        0.727\n    3    446      56    0.604  0.0192       0.5672        0.643\n    4    390      80    0.480  0.0197       0.4429        0.520\n    5    310      55    0.395  0.0192       0.3588        0.434\n    6    246      56    0.305  0.0182       0.2712        0.343\n    7    190      31    0.255  0.0173       0.2234        0.291\n    8    159      25    0.215  0.0163       0.1853        0.250\n    9    134      23    0.178  0.0152       0.1506        0.211\n   10    111      10    0.162  0.0147       0.1357        0.194\n   11    101      23    0.125  0.0132       0.1018        0.154\n   12     78      13    0.104  0.0122       0.0829        0.131\n\n                Temp$Temperature=30 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    610     210  0.65574 0.01924      0.61910       0.6945\n    2    400      85  0.51639 0.02023      0.47822       0.5576\n    3    315     115  0.32787 0.01901      0.29265       0.3673\n    4    200      85  0.18852 0.01584      0.15991       0.2223\n    5    115      38  0.12623 0.01345      0.10244       0.1555\n    6     77      32  0.07377 0.01058      0.05569       0.0977\n    7     45      22  0.03770 0.00771      0.02525       0.0563\n    8     23       7  0.02623 0.00647      0.01617       0.0425\n    9     16       5  0.01803 0.00539      0.01004       0.0324\n   10     11       3  0.01311 0.00461      0.00659       0.0261\n   11      8       4  0.00656 0.00327      0.00247       0.0174\n   12      4       1  0.00492 0.00283      0.00159       0.0152\n\n\n###Showing The Data (using Flextable!):\n\n\nShow the code\nlibrary(flextable); library(tidyverse); library(webshot2)\nuse_df_printer()\nBonds <- read_csv(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/Anthropogenic.csv\")\nBonds\n\n\n\nTemperatureSedimentTimeStatusnumericnumericnumericnumeric263011263031263051263050263050263050263050263050263050263050n: 80\n\n\n###Showing The Data (using DT!):\n\n\nShow the code\nlibrary(DT)\n#install.packages(\"DT\")\nitems <- readr::read_csv(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/Natural.csv\")\ndatatable(head(items))"
  }
]