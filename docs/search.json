[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Corey Cassell",
    "section": "",
    "text": "A Portland Oregon based engineer and masters in data science candidate with 7+ years of broad-based experience utilizing advanced data and analytical tools, building production grade metrics and visualizations, while integrating industrial engineering principles to drive continuous improvement for domestic and international customers."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Corey Cassell",
    "section": "Education",
    "text": "Education\nWillamette University\nMasters in Data Science (MSDS) Planned Graduation August 2023\nCalifornia Polytechnical State University, San Luis Obispo\nBachelor of Science, Industrial Engineering"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Corey Cassell",
    "section": "Skills",
    "text": "Skills\nSOFTWARE, PLATFORMS, AND PROGRAMMING\n\nSQL – Accessing, cleaning, and joining high volume data in a fast-paced manufacturing environment.\nR – Crafting visualizations through base\nR and ggplot. Writing web scraping scripts. Tuning and implementing machine learning models.\nTableau – Design, build, implementation of production facing metrics and analytical tools for manager and executive level communication. Improving user access and understanding of production performance to enable better allocation of resources.\nAccess/Excel/VBA – creating automated metrics and scrips to answer emergent questions.\nBASH – Basic file navigation and manipulation. Remote server interaction. Docker file building and deployment.\n\nGitHub – Project management and version control.\nPython – Basic level understanding and capability.\n\nDATA SCIENCE\n\nData Visualization – Using R and Tableau to analyze and interpret business anomalies through uncovering hidden trends.\nData Engineering – Employing the ETL process in the setup, design, and management of PostgreSQL server/database relationship. joining of tables from cross platform databases (Teradata/Oracle/Microsoft SQL Server) and transforming information into useable content.\n\nMachine learning – Leveraging linear regression-based models to predict next day manufacturing performance for more efficient current day resource allocation.\nSurvival Analysis and survival curve plotting to predict product life expectancy"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Corey Cassell",
    "section": "Experience",
    "text": "Experience\nINDUSTRIAL ENGINEER III, BOEING October 2016 – Current\nBOEING PORTLAND, OR, USA – SUPPORT FOR ACTUATION SYSTEMS MANUFACTURING TEAM\n\nTeam focal for analytics and metrics development using Tableau, Access, and Excel platforms\nGeneration of Tableau dashboards through the integration of multiplatform SQL query building\nDevelopment of statistical standard processes for production performance visibility including data cleaning, preparation, analysis, and site leadership level communication\nAutomated daily report design, construction, and support\nMachining cell layout design and implementation\nDetailed production schedule recovery planning\nPlant staffing and machining capacity analysis in dynamic demand environment\n\nBOEING SHEFFIELD, UK – 7 MONTHS TEMPORARY ASSIGNMENT\n\nUser story design and development for web metric deployment\nOperation level time study analysis enabling targeted problem solving for cycle time reduction\nTraining and support of Sheffield IE team\nLean 3P (production preparation process) workshop support for transmission housings\nThird party supplier capacity and supply chain buffering analysis\nWork statement and movement scenario analysis providing solutions for over capacity cells\nNew product cross site production planning and integration"
  },
  {
    "objectID": "AnotherNewTab.html",
    "href": "AnotherNewTab.html",
    "title": "This is a tab that I am testing",
    "section": "",
    "text": "About this site: you don’t need to know anyting about htis site\n\n1 + 1\n\n[1] 2\n\n#just testing out some changes!!!\n#more changes\n#another test\n\n::: {layout-ncol=2} This fish is pretty neat \n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nWarning in gg_record(dir = \"tidytuesday-temp\", device = \"png\", width = 8, :\nWriting to a folder that already exists. gg_playback may use more files than\nintended!\n\n\nRows: 18215 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): tag_id, study_name\ndbl  (5): event_id, location_long, location_lat, ground_speed, height_above_...\nlgl  (3): visible, algorithm_marked_outlier, manually_marked_outlier\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 101 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): tag_id, animal_id, animal_taxon, animal_reproductive_condition, an...\ndbl  (4): prey_p_month, hrs_indoors, n_cats, age_years\nlgl  (4): hunt, food_dry, food_wet, food_other\ndttm (2): deploy_on_date, deploy_off_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nJoining, by = \"tag_id\"\n\n\n\na"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corey Cassell",
    "section": "",
    "text": "A Portland Oregon based engineer and masters in data science candidate with 7+ years of broad-based experience utilizing advanced data and analytical tools, building production grade metrics and visualizations, while integrating industrial engineering principles to drive continuous improvement for domestic and international customers."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Corey Cassell",
    "section": "Education",
    "text": "Education\nWillamette University\nMasters in Data Science (MSDS) Planned Graduation August 2023\nCalifornia Polytechnical State University, San Luis Obispo\nBachelor of Science, Industrial Engineering"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Corey Cassell",
    "section": "Skills",
    "text": "Skills\nSOFTWARE, PLATFORMS, AND PROGRAMMING\n\nSQL – Accessing, cleaning, and joining high volume data in a fast-paced manufacturing environment.\nR – Crafting visualizations through base\nR and ggplot. Writing web scraping scripts. Tuning and implementing machine learning models.\nTableau – Design, build, implementation of production facing metrics and analytical tools for manager and executive level communication. Improving user access and understanding of production performance to enable better allocation of resources.\nAccess/Excel/VBA – creating automated metrics and scrips to answer emergent questions.\nBASH – Basic file navigation and manipulation. Remote server interaction. Docker file building and deployment.\n\nGitHub – Project management and version control.\nPython – Basic level understanding and capability.\n\nDATA SCIENCE\n\nData Visualization – Using R and Tableau to analyze and interpret business anomalies through uncovering hidden trends.\nData Engineering – Employing the ETL process in the setup, design, and management of PostgreSQL server/database relationship. joining of tables from cross platform databases (Teradata/Oracle/Microsoft SQL Server) and transforming information into useable content.\n\nMachine learning – Leveraging linear regression-based models to predict next day manufacturing performance for more efficient current day resource allocation.\nSurvival Analysis and survival curve plotting to predict product life expectancy"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Corey Cassell",
    "section": "Experience",
    "text": "Experience\nINDUSTRIAL ENGINEER III, BOEING October 2016 – Current\nBOEING PORTLAND, OR, USA – SUPPORT FOR ACTUATION SYSTEMS MANUFACTURING TEAM\n\nTeam focal for analytics and metrics development using Tableau, Access, and Excel platforms\nGeneration of Tableau dashboards through the integration of multiplatform SQL query building\nDevelopment of statistical standard processes for production performance visibility including data cleaning, preparation, analysis, and site leadership level communication\nAutomated daily report design, construction, and support\nMachining cell layout design and implementation\nDetailed production schedule recovery planning\nPlant staffing and machining capacity analysis in dynamic demand environment\n\nBOEING SHEFFIELD, UK – 7 MONTHS TEMPORARY ASSIGNMENT\n\nUser story design and development for web metric deployment\nOperation level time study analysis enabling targeted problem solving for cycle time reduction\nTraining and support of Sheffield IE team\nLean 3P (production preparation process) workshop support for transmission housings\nThird party supplier capacity and supply chain buffering analysis\nWork statement and movement scenario analysis providing solutions for over capacity cells\nNew product cross site production planning and integration"
  },
  {
    "objectID": "posts/Project-1/index.html",
    "href": "posts/Project-1/index.html",
    "title": "ProjectsTEST",
    "section": "",
    "text": "An example of a KNN model that we are building and improving in class. The objective is to accurately predict the regional origin of a bottle of wine.\n\n\n\n\n\n\n\n\n\n\nAccuracy is very high for Burgundy, California, and Oregon regions. Smaller regions performed substantially worse, potentially due to significantly lower availability of data.\n\n\nShow the code\n#PROJECT 1 Feature Engineering\nset.seed(504)\nwine_index <- createDataPartition(wine$province, p = 0.8, list = FALSE)\ntrain <- wine[ wine_index, ]\ntest <- wine[-wine_index, ]\n\n#10 fold cross validation\ncontrol <- trainControl(\n                           method = \"repeatedcv\",\n                           number = 3\n                           )\n\nfit <- train(province ~ .,\n             data = train, \n             method = \"knn\",#knn\n             tuneLength = 20, #the 30 different values for K\n             metric = \"Kappa\",\n             trControl = control)\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               213          8                 0           3        2\n  California               5        689                 9          15        8\n  Casablanca_Valley        0          0                 8           0        1\n  Marlborough              0          1                 0          10        0\n  New_York                 0          0                 0           1        4\n  Oregon                  20         93                 9          16       11\n                   Reference\nPrediction          Oregon\n  Burgundy              16\n  California           131\n  Casablanca_Valley      1\n  Marlborough            0\n  New_York               0\n  Oregon               399\n\nOverall Statistics\n                                          \n               Accuracy : 0.7908          \n                 95% CI : (0.7705, 0.8101)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6678          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.8950            0.8710                 0.307692\nSpecificity                   0.9798            0.8095                 0.998786\nPos Pred Value                0.8802            0.8040                 0.800000\nNeg Pred Value                0.9825            0.8750                 0.989176\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1273            0.4118                 0.004782\nDetection Prevalence          0.1447            0.5123                 0.005977\nBalanced Accuracy             0.9374            0.8403                 0.653239\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.222222        0.153846        0.7294\nSpecificity                    0.999386        0.999393        0.8677\nPos Pred Value                 0.909091        0.800000        0.7281\nNeg Pred Value                 0.978941        0.986811        0.8684\nPrevalence                     0.026898        0.015541        0.3270\nDetection Rate                 0.005977        0.002391        0.2385\nDetection Prevalence           0.006575        0.002989        0.3276\nBalanced Accuracy              0.610804        0.576619        0.7986\n\n\n\n\n\nThis graphs kappa performance of the model over a range of K values. Kappa is a measure of model performance against random chance classification. We want to achieve the highest kappa value possible.\n\n\nShow the code\nggplot(fit, metric=\"Kappa\")"
  },
  {
    "objectID": "posts/Project-1/index.html#project-3-using-tables",
    "href": "posts/Project-1/index.html#project-3-using-tables",
    "title": "ProjectsTEST",
    "section": "Project 3: Using Tables",
    "text": "Project 3: Using Tables\nThis is a study of Natural vs Anthropogenic soils for\n\n\nShow the code\n#data repo: https://github.com/bobbyjy/MyData\n\n#load dataset into git repo\nthePath=\"https://raw.githubusercontent.com/bobbyjy/MyData/main/\"\n\n#update with file name for project 1\nAnth=read_csv(paste(thePath,\"Anthropogenic.csv\",sep=\"/\"))\n\n\nRows: 80 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Temperature, Sediment, Time, Status\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\n#load dataset into git repo\nthePath=\"https://raw.githubusercontent.com/bobbyjy/MyData/main/\"\n\n#update with file name for project 1\nNat=read_csv(paste(thePath,\"Natural.csv\",sep=\"/\"))\n\n\nRows: 1176 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Temperature, Sediment, Time, Status\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow the code\nAnth<-Anth %>% mutate(anthro=1)\nNat<-Nat %>% mutate(anthro=0)\ncoraldf <- dplyr::combine(Anth,Nat)\n\n\nWarning: `combine()` was deprecated in dplyr 1.0.0.\nℹ Please use `vctrs::vec_c()` instead.\n\n\n#| code-fold: true #| code-summary: “Show the code” #comparing temperature and survival Temp <- coraldf %>% select(Temperature, Time, Status) km.Temp =survfit(Surv(Temp\\(Time,Temp\\)Status)~Temp$Temperature)\np <- ggsurvplot(fit=km.Temp, data=Temp, legend = “bottom”, risk.table = F,conf.int=F, legend.title = “Study Temperatures:”) + labs( title=“Visualizing the Impact of Global Warming on Coral Populations”, subtitle = “Observing Coral Survival in 26C and 30C Groups”, x=“Time Until Coral Death (Weeks)” )\np$plot + scale_colour_manual(values = c(“turquoise1”, “red”))+ theme(panel.background = element_rect(fill = “grey12”), plot.background = element_rect(fill=“grey12”), axis.text.x=element_text(colour=“white”), axis.text.y=element_text(colour=“white”), plot.title = element_text(colour = “white”), axis.title.x = element_text(colour = “white”), axis.title.y = element_text(colour = “white”), axis.line.x = element_line(color=“white”), axis.line.y = element_line(color=“white”), legend.key = element_rect(fill = “grey12”), legend.background = element_rect(fill=“grey12”), legend.text = element_text(color = “white”), legend.title = element_text(color = “white”), plot.subtitle = element_text(color=“white”))+ annotate(“text”, x = 2, y = .25, label = “30C Group”, color = “red”, fontface = “bold”)+ annotate(“text”, x = 6, y = .65, label = “26C Group”, color = “turquoise1”, fontface = “bold”)\n#add annotation labels for the temp groups #fix legend background # setwd(“C:\\Users\\corey\\OneDrive\\Documents\\GitHub\\599_Project1”) # ggsave(“GlobalWarmingImpact.png”, dpi = 300)\nsurvdiff(Surv(Temp\\(Time,Temp\\)Status)~Temp$Temperature)\nsummary(km.Temp)"
  },
  {
    "objectID": "posts/Project-2/index.html",
    "href": "posts/Project-2/index.html",
    "title": "Tables!",
    "section": "",
    "text": "This is a study of Natural vs Anthropogenic soils for\n\n#data repo: https://github.com/bobbyjy/MyData\n\n#load dataset into git repo\nthePath=\"https://raw.githubusercontent.com/bobbyjy/MyData/main/\"\n\n#update with file name for project 1\nAnth=read_csv(paste(thePath,\"Anthropogenic.csv\",sep=\"/\"))\n\nRows: 80 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Temperature, Sediment, Time, Status\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#load dataset into git repo\nthePath=\"https://raw.githubusercontent.com/bobbyjy/MyData/main/\"\n\n#update with file name for project 1\nNat=read_csv(paste(thePath,\"Natural.csv\",sep=\"/\"))\n\nRows: 1176 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): Temperature, Sediment, Time, Status\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nAnth<-Anth %>% mutate(anthro=1)\nNat<-Nat %>% mutate(anthro=0)\ncoraldf <- dplyr::combine(Anth,Nat)\n\nWarning: `combine()` was deprecated in dplyr 1.0.0.\nℹ Please use `vctrs::vec_c()` instead.\n\n\n#comparing temperature and survival Temp <- coraldf %>% select(Temperature, Time, Status) km.Temp =survfit(Surv(Temp\\(Time,Temp\\)Status)~Temp$Temperature)\np <- ggsurvplot(fit=km.Temp, data=Temp, legend = “bottom”, risk.table = F,conf.int=F, legend.title = “Study Temperatures:”) + labs( title=“Visualizing the Impact of Global Warming on Coral Populations”, subtitle = “Observing Coral Survival in 26C and 30C Groups”, x=“Time Until Coral Death (Weeks)” )\np$plot + scale_colour_manual(values = c(“turquoise1”, “red”))+ theme(panel.background = element_rect(fill = “grey12”), plot.background = element_rect(fill=“grey12”), axis.text.x=element_text(colour=“white”), axis.text.y=element_text(colour=“white”), plot.title = element_text(colour = “white”), axis.title.x = element_text(colour = “white”), axis.title.y = element_text(colour = “white”), axis.line.x = element_line(color=“white”), axis.line.y = element_line(color=“white”), legend.key = element_rect(fill = “grey12”), legend.background = element_rect(fill=“grey12”), legend.text = element_text(color = “white”), legend.title = element_text(color = “white”), plot.subtitle = element_text(color=“white”))+ annotate(“text”, x = 2, y = .25, label = “30C Group”, color = “red”, fontface = “bold”)+ annotate(“text”, x = 6, y = .65, label = “26C Group”, color = “turquoise1”, fontface = “bold”)\n#add annotation labels for the temp groups #fix legend background # setwd(“C:\\Users\\corey\\OneDrive\\Documents\\GitHub\\599_Project1”) # ggsave(“GlobalWarmingImpact.png”, dpi = 300)\nsurvdiff(Surv(Temp\\(Time,Temp\\)Status)~Temp$Temperature)\nsummary(km.Temp)"
  },
  {
    "objectID": "posts/Project-3/index.html#plot1",
    "href": "posts/Project-3/index.html#plot1",
    "title": "Interactive Plots",
    "section": "plot1!!!!",
    "text": "plot1!!!!\n\n\nShow the code\nds1 <- read_csv(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/job_retention-hopa.txt\")\n\nds1\n\n\n# A tibble: 3,770 × 7\n   gender field                  level  sentiment intention  left month\n   <chr>  <chr>                  <chr>      <dbl>     <dbl> <dbl> <dbl>\n 1 M      Public/Government      High           3         8     1     1\n 2 F      Finance                Low            8         4     0    12\n 3 M      Education and Training Medium         7         7     1     5\n 4 M      Finance                Low            8         4     0    12\n 5 M      Finance                High           7         6     1     1\n 6 F      Health                 Medium         6        10     1     2\n 7 M      Education and Training Medium         8         2     0    12\n 8 M      Finance                Low            7         9     0    12\n 9 M      Public/Government      High           7         6     1     5\n10 M      Law                    Low            7         4     0    12\n# … with 3,760 more rows"
  },
  {
    "objectID": "posts/Project-3/index.html#plot2",
    "href": "posts/Project-3/index.html#plot2",
    "title": "Interactive Plots",
    "section": "plot2!!!!",
    "text": "plot2!!!!\n\n\nShow the code\nds2 <- read_rds(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/pinot.rds\")\n\na<-ds2 %>% \n  ggplot(aes(price,points))+\n  geom_point()\n\nggplotly(a)"
  },
  {
    "objectID": "Projects2.html",
    "href": "Projects2.html",
    "title": "Some cool things…",
    "section": "",
    "text": "ProjectsTEST\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTables!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPresentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Visualization.html",
    "href": "Visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "My portfolio! I’ll be storing examples of my work and skillset in this location.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nggsurvplot(fit=km, data=ds,\n           legend = \"bottom\", \n           legend.title = \"Treatment Group\",\n           legend.labs = c(\"Combination\", \"Patch Only\"),\nrisk.table = F,conf.int=T) +\n    labs(\n        title=\"Survival Curve for Pharmaco Study\",\n        x=\"Time to Relapse)\"\n    )"
  },
  {
    "objectID": "posts/Project-4/index.html#plot-1",
    "href": "posts/Project-4/index.html#plot-1",
    "title": "Presentation",
    "section": "Plot 1",
    "text": "Plot 1\n\n\nShow the code\nds1 <- read_csv(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/job_retention-hopa.txt\")\n\ndatatable(head(ds1))"
  },
  {
    "objectID": "posts/Project-4/index.html#plot-2",
    "href": "posts/Project-4/index.html#plot-2",
    "title": "Presentation",
    "section": "Plot 2",
    "text": "Plot 2\n\n\nShow the code\nds2 <- read_rds(\"https://raw.githubusercontent.com/bobbyjy/MyData/main/pinot.rds\")\n\ndatatable(head(ds2, n = 3))\n\n\n\n\n\n\n\nShow the code\na<-ds2 %>% \n  ggplot(aes(price,points))+\n  geom_point()\n\nggplotly(a)"
  }
]