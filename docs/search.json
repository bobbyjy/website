[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Corey Cassell",
    "section": "",
    "text": "A Portland Oregon based engineer and masters in data science candidate with 7+ years of broad-based experience utilizing advanced data and analytical tools, building production grade metrics and visualizations, while integrating industrial engineering principles to drive continuous improvement for domestic and international customers."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Corey Cassell",
    "section": "Education",
    "text": "Education\nWillamette University\nMasters in Data Science (MSDS) Planned Graduation August 2023\nCalifornia Polytechnical State University, San Luis Obispo\nBachelor of Science, Industrial Engineering"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Corey Cassell",
    "section": "Skills",
    "text": "Skills\nSOFTWARE, PLATFORMS, AND PROGRAMMING\n\nSQL – Accessing, cleaning, and joining high volume data in a fast-paced manufacturing environment.\nR – Crafting visualizations through base\nR and ggplot. Writing web scraping scripts. Tuning and implementing machine learning models.\nTableau – Design, build, implementation of production facing metrics and analytical tools for manager and executive level communication. Improving user access and understanding of production performance to enable better allocation of resources.\nAccess/Excel/VBA – creating automated metrics and scrips to answer emergent questions.\nBASH – Basic file navigation and manipulation. Remote server interaction. Docker file building and deployment.\n\nGitHub – Project management and version control.\nPython – Basic level understanding and capability.\n\nDATA SCIENCE\n\nData Visualization – Using R and Tableau to analyze and interpret business anomalies through uncovering hidden trends.\nData Engineering – Employing the ETL process in the setup, design, and management of PostgreSQL server/database relationship. joining of tables from cross platform databases (Teradata/Oracle/Microsoft SQL Server) and transforming information into useable content.\n\nMachine learning – Leveraging linear regression-based models to predict next day manufacturing performance for more efficient current day resource allocation.\nSurvival Analysis and survival curve plotting to predict product life expectancy"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Corey Cassell",
    "section": "Experience",
    "text": "Experience\nINDUSTRIAL ENGINEER III, BOEING October 2016 – Current\nBOEING PORTLAND, OR, USA – SUPPORT FOR ACTUATION SYSTEMS MANUFACTURING TEAM\n\nTeam focal for analytics and metrics development using Tableau, Access, and Excel platforms\nGeneration of Tableau dashboards through the integration of multiplatform SQL query building\nDevelopment of statistical standard processes for production performance visibility including data cleaning, preparation, analysis, and site leadership level communication\nAutomated daily report design, construction, and support\nMachining cell layout design and implementation\nDetailed production schedule recovery planning\nPlant staffing and machining capacity analysis in dynamic demand environment\n\nBOEING SHEFFIELD, UK – 7 MONTHS TEMPORARY ASSIGNMENT\n\nUser story design and development for web metric deployment\nOperation level time study analysis enabling targeted problem solving for cycle time reduction\nTraining and support of Sheffield IE team\nLean 3P (production preparation process) workshop support for transmission housings\nThird party supplier capacity and supply chain buffering analysis\nWork statement and movement scenario analysis providing solutions for over capacity cells\nNew product cross site production planning and integration"
  },
  {
    "objectID": "AnotherNewTab.html",
    "href": "AnotherNewTab.html",
    "title": "This is a tab that I am testing",
    "section": "",
    "text": "About this site: you don’t need to know anyting about htis site\n\n1 + 1\n\n[1] 2\n\n#just testing out some changes!!!\n#more changes\n#another test\n\n::: {layout-ncol=2} This fish is pretty neat \n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   1.0.1 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nWarning in gg_record(dir = \"tidytuesday-temp\", device = \"png\", width = 8, :\nWriting to a folder that already exists. gg_playback may use more files than\nintended!\n\n\nRows: 18215 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): tag_id, study_name\ndbl  (5): event_id, location_long, location_lat, ground_speed, height_above_...\nlgl  (3): visible, algorithm_marked_outlier, manually_marked_outlier\ndttm (1): timestamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 101 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): tag_id, animal_id, animal_taxon, animal_reproductive_condition, an...\ndbl  (4): prey_p_month, hrs_indoors, n_cats, age_years\nlgl  (4): hunt, food_dry, food_wet, food_other\ndttm (2): deploy_on_date, deploy_off_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nJoining, by = \"tag_id\"\n\n\n\na"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "My portfolio! I’ll be storing examples of my work and skill-set in this location.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "As an introductory project aimed at exploring the functionality of Quarto and Github I have replicated an entry from #tidytuesday For this example I am using code from a submission by @ryanahart with (code: https://github.com/curatedmess/TidyTuesday/blob/main/2023/01312023/cats.R)"
  },
  {
    "objectID": "Visualization.html",
    "href": "Visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "My portfolio! I’ll be storing examples of my work and skillset in this location.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nggsurvplot(fit=km, data=ds,\n           legend = \"bottom\", \n           legend.title = \"Treatment Group\",\n           legend.labs = c(\"Combination\", \"Patch Only\"),\nrisk.table = F,conf.int=T) +\n    labs(\n        title=\"Survival Curve for Pharmaco Study\",\n        x=\"Time to Relapse)\"\n    )"
  },
  {
    "objectID": "Projects.html#project-2",
    "href": "Projects.html#project-2",
    "title": "Projects",
    "section": "Project 2:",
    "text": "Project 2:\nAn example of a KNN model that we are building and improving in class. The objective is to accurately predict the regional origin of a bottle of wine.\n\nUnderstanding most frequently used regional descriptors…\n\n\n\n\n\n\n\nHow does the model perform across provinces?\nAccuracy is very high for Burgundy, California, and Oregon regions. Smaller regions performed substantially worse, potentially due to significantly lower availability of data.\n\n#PROJECT 1 Feature Engineering\nset.seed(504)\nwine_index <- createDataPartition(wine$province, p = 0.8, list = FALSE)\ntrain <- wine[ wine_index, ]\ntest <- wine[-wine_index, ]\n\n#10 fold cross validation\ncontrol <- trainControl(\n                           method = \"repeatedcv\",\n                           number = 5,\n                           repeats = 3\n                           )\n\nfit <- train(province ~ .,\n             data = train, \n             method = \"knn\",#knn\n             tuneLength = 20, #the 30 different values for K\n             metric = \"Kappa\",\n             trControl = control)\n\nconfusionMatrix(predict(fit, test),factor(test$province))\n\nConfusion Matrix and Statistics\n\n                   Reference\nPrediction          Burgundy California Casablanca_Valley Marlborough New_York\n  Burgundy               214          7                 0           4        2\n  California               4        699                 7          16       10\n  Casablanca_Valley        0          0                 8           0        1\n  Marlborough              0          1                 0           7        1\n  New_York                 0          0                 0           1        2\n  Oregon                  20         84                11          17       10\n                   Reference\nPrediction          Oregon\n  Burgundy              16\n  California           136\n  Casablanca_Valley      1\n  Marlborough            0\n  New_York               0\n  Oregon               394\n\nOverall Statistics\n                                          \n               Accuracy : 0.7914          \n                 95% CI : (0.7711, 0.8106)\n    No Information Rate : 0.4728          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6678          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Burgundy Class: California Class: Casablanca_Valley\nSensitivity                   0.8992            0.8837                 0.307692\nSpecificity                   0.9798            0.8039                 0.998786\nPos Pred Value                0.8807            0.8016                 0.800000\nNeg Pred Value                0.9832            0.8851                 0.989176\nPrevalence                    0.1423            0.4728                 0.015541\nDetection Rate                0.1279            0.4178                 0.004782\nDetection Prevalence          0.1452            0.5212                 0.005977\nBalanced Accuracy             0.9395            0.8438                 0.653239\n                     Class: Marlborough Class: New_York Class: Oregon\nSensitivity                    0.155556        0.076923        0.7203\nSpecificity                    0.998771        0.999393        0.8739\nPos Pred Value                 0.777778        0.666667        0.7351\nNeg Pred Value                 0.977163        0.985629        0.8654\nPrevalence                     0.026898        0.015541        0.3270\nDetection Rate                 0.004184        0.001195        0.2355\nDetection Prevalence           0.005380        0.001793        0.3204\nBalanced Accuracy              0.577164        0.538158        0.7971\n\n\n\n\nWhich K value is ideal?\nThis graphs kappa performance of the model over a range of K values. Kappa is a measure of model performance against random chance classification. We want to achieve the highest kappa value possible.\n\nggplot(fit, metric=\"Kappa\")"
  }
]